{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2be34067-ec94-42fd-b313-ec6db1cfc760",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b3984a-918a-44f1-a391-7aa2abd43480",
   "metadata": {},
   "source": [
    "## Using PyTorch to handle tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0655fb06-8c00-418b-8b69-c6367ee2a0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b596b01-7372-4465-8406-7bdbf4eaf79a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7630,  0.4577, -0.1429, -0.1219],\n",
       "        [-1.4268, -1.8814,  0.2850,  0.7874],\n",
       "        [ 0.0273, -0.4959, -0.7572,  2.6761]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(12, dtype=torch.float32) # The arange(n) function creates a tensor with values from 1 to n - 1.\n",
    "x\n",
    "x.numel()\n",
    "X = x.reshape(3, 4) # Rearrange the tensor as a tensor of size 3x4.\n",
    "torch.zeros((2, 3, 4)) # Create a 2x3x4 tensor with just zeros.\n",
    "torch.ones((2, 3, 4)) # Create a 2x3x4 tensor with just ones.\n",
    "torch.randn(3, 4) # Create a 3x4 tensor with random values from a standard normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f46f43-4291-4c66-8292-615254abf6af",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Indexing and slicing allow to access elements from a tensor.\n",
    "Indexing can also be used to manipulate and modify a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc76d724-9da5-487d-acf8-84ab7acd465c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5., 17.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[-1] # Indexing: access one element.\n",
    "X[1:3] # Slicing: access more elements\n",
    "X[1, 2] = 17 # Change X[1, 2] from 6 to 17.\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cac5478-b45e-48ac-a273-45d1139eab50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [1, 2],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(x) # Apply e^x on each entry of x.\n",
    "x = torch.tensor([1, 2, 4, 8])\n",
    "y = torch.tensor([2, 2, 2, 2])\n",
    "x + y # Point-wise sum.\n",
    "x - y # Point-wise subtraction.\n",
    "x / y # Point-wise division.\n",
    "x * y # Point-wise multiplication.\n",
    "\n",
    "X = torch.arange(12, dtype=torch.float32).reshape((3, 4))\n",
    "Y = torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1) # Concatenation along the x and y axes.\n",
    "\n",
    "X == Y # Binary tensor that checks whether X[i, j] = Y[i, j].\n",
    "\n",
    "X.sum() # Summing elements in a tensor returns a 1x1 tensor with the result.\n",
    "\n",
    "a = torch.arange(3).reshape((3, 1))\n",
    "b = torch.arange(2).reshape((1, 2))\n",
    "a, b\n",
    "a + b # The shape of the result is maximised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd58f343-60a7-4ae2-a567-84387b23d79c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([3.5])\n",
    "a.item() # Convert a 1x1 tensor into a scalar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475627ec-a3a8-4bb9-a472-fbec7cd4052a",
   "metadata": {},
   "source": [
    "## AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c297351-8e27-4bea-aa30-098dfcb09ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/d2l-ai/d2l-pytorch-colab.git\n",
    "%cd 'd2l-pytorch-colab'\n",
    "!pip install -e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b41fc64-acf6-44bd-9c72-2de29af32df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a4fde6-93dc-4553-af9a-e5633a0945e6",
   "metadata": {},
   "source": [
    "AlexNet is an 8-layer convolutional neural network that, while shallower than more modern models, is still vable to achieve good classification performance, although, nowadays, it is less used due to the big memory overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17692f78-1f84-47ac-b4a4-af6420a22a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(d2l.Classifier):\n",
    "    def __init__(self, lr=0.1, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LazyConv2d(96, kernel_size=11, stride=4, padding=1),\n",
    "            nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.LazyConv2d(256, kernel_size=5, padding=2), nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.LazyConv2d(384, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.LazyConv2d(384, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.LazyConv2d(256, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2), nn.Flatten(),\n",
    "            nn.LazyLinear(4096), nn.ReLU(), nn.Dropout(p=0.5),\n",
    "            nn.LazyLinear(4096), nn.ReLU(),nn.Dropout(p=0.5),\n",
    "            nn.LazyLinear(num_classes))\n",
    "        self.net.apply(d2l.init_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca5816e-b323-4a53-b79b-b39475c356f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "AlexNet().layer_summary((1, 1, 224, 224)) # Summary of AlexNet's behaviour and output shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6c5d49-f0ec-4ffd-bc2d-18279c909f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AlexNet(lr=0.01) # Create an instance of the model with learning rate 0.01.\n",
    "data = d2l.FashionMNIST(batch_size=128, resize=(224, 224)) # For simplicity, perform training on the FashionMNIST dataset.\n",
    "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
    "trainer.fit(model, data)\n",
    "\n",
    "'''\n",
    "Overall, this instance manages to achieve very good classification accuracy throughout the epochs.\n",
    "In particular, most of the loss is handled during the first two epochs and then steadily decreases from the next one.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9a99c0-d91e-48b8-8926-7eb95e4a9f5e",
   "metadata": {},
   "source": [
    "Questions:\n",
    "1. **Analyze the computational properties of AlexNet.**\n",
    "   1. **Compute the memory footprint for convolutions and fully connected layers.** <br>\n",
    "      Most of the memory footprint is associated to the first fully connected layer as it needs to implement very large weight matrices for the following fully connected layers. <br>\n",
    "      Generally speaking, it is possible to measure the memory footprint in terms of the number of required parameters, which, in the case of a convolutional layer, can be computed as $(F^2C + 1)K$ or, in the case of a linear layer, as $FCIO$. <br>\n",
    "      Therefore, most of the work will be carried out at the fully connected layers.\n",
    "   2. **Calculate the computational cost for the convolutions and the fully connected layers.** <br>\n",
    "      Despite its simple architecture, AlexNet requires approximately 60 million parameters, resulting in a very high computational cost.\n",
    "   3. **How does the memory affect computation?** <br>\n",
    "      From a general point of view, the memory overhead of AlexNet introduces an additional latency that is most significant during the computations of the fully connected layers.\n",
    "2. **You are a chip designer and need to trade off computation and memory bandwidth. How do you optimize?**\n",
    "3. **Why do engineers no longer report performance benchmarks on AlexNet?**\n",
    "4. **Try increasing the number of epochs when training AlexNet. Compared to LeNet, how do the results differ?**\n",
    "5. **AlexNet may be too complex for he Fashion-MNIST dataset.**\n",
    "   1. **Try simplifying the model to make the training faster, while ensuring that the accuracy does not drop significantly.**\n",
    "   2. **Design a better model that works directly on 28x28 image.**\n",
    "6. **Modify the batch size and observe the changes in throughput, accuracy and GPU memory.**\n",
    "7. **Apply dropout and ReLU to LeNet-5: does it improve?**\n",
    "8. **Can you make AlexNet overfit?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c295496a-1333-48c9-b4a2-3d8b740e7ef4",
   "metadata": {},
   "source": [
    "## VGGNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef79461-74be-4ccc-9a17-df6217955caa",
   "metadata": {},
   "source": [
    "VGGNet is a convolutional neural network that tries to implement repeated layer patterns as blocks consisting of multiple convolutional layers followed by a max pooling layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f4a2f3-7ccf-417b-85ce-533df59cfeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_block(num_convs, out_channels):\n",
    "    layers = []\n",
    "    for _ in range(num_convs):\n",
    "        layers.append(nn.LazyConv2d(out_channels, kernel_size=3, padding=1))\n",
    "        layers.append(nn.ReLU())\n",
    "    layers.append(nn.MaxPool2d(kernel_size=2,stride=2))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a1ea73-858c-42a6-8de5-689ec0ed66e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(d2l.Classifier):\n",
    "    def __init__(self, arch, lr=0.1, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        conv_blks = []\n",
    "        for (num_convs, out_channels) in arch:\n",
    "            conv_blks.append(vgg_block(num_convs, out_channels))\n",
    "        self.net = nn.Sequential(\n",
    "            *conv_blks, nn.Flatten(),\n",
    "            nn.LazyLinear(4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "            nn.LazyLinear(4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "            nn.LazyLinear(num_classes))\n",
    "        self.net.apply(d2l.init_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98e8f8e-bc90-4603-a34e-3f00ee2dcc4d",
   "metadata": {},
   "source": [
    "The first-ever implementation of the VGGNet model was VGG-11, which used five convolutional blocks. <br>\n",
    "Of these, the first two blocks implement one convolutional layer each, whereas the other blocks use two convolutional layers each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9680b7cd-5628-4049-a587-b475421a3c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG(arch=((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))).layer_summary(\n",
    "    (1, 1, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baac42e-251a-4b5c-8b2c-7e9a1c27ef4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG(arch=((1, 16), (1, 32), (2, 64), (2, 128), (2, 128)), lr=0.01)\n",
    "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
    "data = d2l.FashionMNIST(batch_size=128, resize=(224, 224)) # For simplicity, perform training on the FashionMNIST dataset.\n",
    "model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
    "trainer.fit(model, data)\n",
    "\n",
    "'''\n",
    "Similarly for AlexNet, this instance manages to achieve very good classification accuracy throughout the epochs.\n",
    "However, since the training and validation losses are very close, the model is considered to be slightly overfitting the data.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c4dc99-1c04-4b5d-844c-b18c1c8af2f8",
   "metadata": {},
   "source": [
    "Questions:\n",
    "1. **Compared with AlexNet, VGG is much slower in terms of computation, and it also needs more GPU memory.**\n",
    "   1. **Compare the number of parameters needed for AlexNet and VGG.**\n",
    "   2. **Compare the number of floating point operations used in the convolutional layers and in the fully connected layers.**\n",
    "   3. **How could you reduce the computational cost created by the fully connected layers?**\n",
    "2. **When displaying the dimensions associated with the various layers of the network, there is information just for eight blocks. Where did the other layers go?**\n",
    "3. **Construct other common models, such as VGG-16 or VGG-19.**\n",
    "4. **Upsampling the resolution in Fashion-MNIST is very wasteful. Try modifying the network architecture and resolution conversion. Can you do so without reducing the accuracy of the network?**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
