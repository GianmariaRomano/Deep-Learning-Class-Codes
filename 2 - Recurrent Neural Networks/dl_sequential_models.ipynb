{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1984c8a7-aea6-4a60-851f-e77f30d93db5",
   "metadata": {},
   "source": [
    "# Sequential Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d0ac37-d974-47b3-af8c-6f97447f224b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install d2l==1.0.3 --no-deps\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "import collections\n",
    "import random\n",
    "import re\n",
    "from d2l import torch as d2l\n",
    "import math\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7cfe6e-ec9d-4f4f-8af6-b1b8232d7859",
   "metadata": {},
   "source": [
    "## Sequential Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1216cc-6293-4503-9d0b-b3a3fc36961e",
   "metadata": {},
   "source": [
    "An autoregressive model is a particular type of sequential model that, given an input, produces a sequence of outputs. <br>\n",
    "The main feature of autoregression lies in the fact that, after processing the first input, the next predictions will be based on the latest output. <br>\n",
    "For this reason, autoregressive predictions can be expressed using the conditional expectation $E[(x_t∣x_{t−1},…,x_1)],$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d842ff-4ff7-4dad-913b-2cf92a973d78",
   "metadata": {},
   "source": [
    "From a broader perspective, a sequence model allows to express the prediction of an input in terms of the previous terms of the sequence, allowing to formulate $P(x_1,…,x_T)=P(x_1)\\prod_{t=2}^T P(x_t∣x_{t−1},…,x_1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770785bf-f5d1-449b-b565-c0a6fa00cef8",
   "metadata": {},
   "source": [
    "Therefore, create a model that works on synthetic data that follow a sinusoidal pattern with a bit of random noise. <br>\n",
    "The model uses a context window $\\tau = 4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2732204e-0f87-447c-9019-bc365472f834",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(d2l.DataModule):\n",
    "    def __init__(self, batch_size=16, T=1000, num_train=600, tau=4):\n",
    "        self.save_hyperparameters()\n",
    "        self.time = torch.arange(1, T + 1, dtype=torch.float32)\n",
    "        self.x = torch.sin(0.01 * self.time) + torch.randn(T) * 0.2\n",
    "\n",
    "data = Data()\n",
    "d2l.plot(data.time, data.x, 'time', 'x', xlim=[1, 1000], figsize=(6, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e88ef70-7e29-47f9-80ff-3af071b10029",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(Data)\n",
    "def get_dataloader(self, train):\n",
    "    features = [self.x[i : self.T-self.tau+i] for i in range(self.tau)]\n",
    "    self.features = torch.stack(features, 1)\n",
    "    self.labels = self.x[self.tau:].reshape((-1, 1))\n",
    "    i = slice(0, self.num_train) if train else slice(self.num_train, None)\n",
    "    return self.get_tensorloader([self.features, self.labels], train, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b408743-c0f2-4cd5-bf3e-b11bc07aa5b8",
   "metadata": {},
   "source": [
    "Train a standard linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8856c4-7432-4aa8-85ed-d9427ece1f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = d2l.LinearRegression(lr=0.01)\n",
    "trainer = d2l.Trainer(max_epochs=5)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef75e8be-1cb9-4bca-9e91-38d05684d468",
   "metadata": {},
   "source": [
    "One-step-ahead prediction. <br>\n",
    "Since training at step $t$ requires looking at $t$ data points, one can use the context window to look at a smaller portion of the data, allowing to avoid overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a25de8-da78-4810-93ac-7bf367f0d36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "onestep_preds = model(data.features).detach().numpy()\n",
    "d2l.plot(data.time[data.tau:], [data.labels, onestep_preds], 'time', 'x',\n",
    "         legend=['labels', '1-step preds'], figsize=(6, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6714a9e9-138f-483a-b3d4-68a049658cb8",
   "metadata": {},
   "source": [
    "Multiple-step-ahead prediction. <br>\n",
    "This idea comes from the strategy adopted during the one-step-ahead prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fac4c74-58ab-4cc4-8114-22789dac7af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "multistep_preds = torch.zeros(data.T)\n",
    "multistep_preds[:] = data.x\n",
    "for i in range(data.num_train + data.tau, data.T):\n",
    "    multistep_preds[i] = model(\n",
    "        multistep_preds[i - data.tau:i].reshape((1, -1)))\n",
    "multistep_preds = multistep_preds.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639c164f-3c92-4a14-a3bf-d64bc316d975",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.plot([data.time[data.tau:], data.time[data.num_train+data.tau:]],\n",
    "         [onestep_preds, multistep_preds[data.num_train+data.tau:]], 'time',\n",
    "         'x', legend=['1-step preds', 'multistep preds'], figsize=(6, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880c27a1-19ef-45d6-bff7-c9b786487b3c",
   "metadata": {},
   "source": [
    "Observe that this approach actually fails due to error terms building up, as shown by the following plots, which indicate the model performance using different context windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cfa142-eb67-445a-83c0-aaff8c058413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_step_pred(k):\n",
    "    features = []\n",
    "    for i in range(data.tau):\n",
    "        features.append(data.x[i : i+data.T-data.tau-k+1])\n",
    "    # The (i+tau)-th element stores the (i+1)-step-ahead predictions.\n",
    "    for i in range(k):\n",
    "        preds = model(torch.stack(features[i : i+data.tau], 1))\n",
    "        features.append(preds.reshape(-1))\n",
    "    return features[data.tau:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973aa0aa-085e-4866-b76d-79a8dd78c566",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = (1, 4, 16, 64)\n",
    "preds = k_step_pred(steps[-1])\n",
    "d2l.plot(data.time[data.tau+steps[-1]-1:],\n",
    "         [preds[k - 1].detach().numpy() for k in steps], 'time', 'x',\n",
    "         legend=[f'{k}-step preds' for k in steps], figsize=(6, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a2457b-2b32-4717-bb7b-e04dd6b2ec1a",
   "metadata": {},
   "source": [
    "## Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9b9dad-5237-4418-b296-feec58d44f9a",
   "metadata": {},
   "source": [
    "Sequence models are often used for language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c3f4f0-f23a-4af2-aae9-794759a6b1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeMachine(d2l.DataModule):\n",
    "    \"\"\"The Time Machine dataset.\"\"\"\n",
    "    def _download(self):\n",
    "        fname = d2l.download(d2l.DATA_URL + 'timemachine.txt', self.root,\n",
    "                             '090b5e7e70c295757f55df93cb0a180b9691891a')\n",
    "        with open(fname) as f:\n",
    "            return f.read()\n",
    "\n",
    "data = TimeMachine()\n",
    "raw_text = data._download()\n",
    "raw_text[:60]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0b05f1-bafe-472c-a0a8-5187ccdba2b1",
   "metadata": {},
   "source": [
    "Start by preprocessing the text by removing spaces and punctuation. <br>\n",
    "Then, tokenize the text into a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef86d906-2f97-480a-aa9a-f522e180e728",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(TimeMachine)\n",
    "def _preprocess(self, text):\n",
    "    return re.sub('[^A-Za-z]+', ' ', text).lower()\n",
    "\n",
    "text = data._preprocess(raw_text)\n",
    "text[:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7353a5-0268-4107-aa17-0d8e29f48b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(TimeMachine)\n",
    "def _tokenize(self, text):\n",
    "    return list(text)\n",
    "\n",
    "tokens = data._tokenize(text)\n",
    "','.join(tokens[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1154ee0d-2aae-4869-9d2f-1cd3124768fc",
   "metadata": {},
   "source": [
    "Now, create a vocabulary that maps each token into a numerical index. <br>\n",
    "For simplicity, it can come in handy to drop rare/unusual terms, which will instead be mapped to the \"unk\" token to indicate an unknown value associated to that term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f4d9fe-c4a9-4a04-9f94-73b3ddc64239",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    \"\"\"Vocabulary for text.\"\"\"\n",
    "    def __init__(self, tokens=[], min_freq=0, reserved_tokens=[]):\n",
    "        # Flatten a 2D list if needed.\n",
    "        if tokens and isinstance(tokens[0], list):\n",
    "            tokens = [token for line in tokens for token in line]\n",
    "        # Count token frequencies.\n",
    "        counter = collections.Counter(tokens)\n",
    "        self.token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
    "                                  reverse=True)\n",
    "        # The list of unique tokens.\n",
    "        self.idx_to_token = list(sorted(set(['<unk>'] + reserved_tokens + [\n",
    "            token for token, freq in self.token_freqs if freq >= min_freq])))\n",
    "        self.token_to_idx = {token: idx\n",
    "                             for idx, token in enumerate(self.idx_to_token)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if hasattr(indices, '__len__') and len(indices) > 1:\n",
    "            return [self.idx_to_token[int(index)] for index in indices]\n",
    "        return self.idx_to_token[indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self):  # Index for the unknown token.\n",
    "        return self.token_to_idx['<unk>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a08e02-d256-44c0-ab4b-34ad7aa5714c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab(tokens)\n",
    "indices = vocab[tokens[:10]]\n",
    "print('indices:', indices)\n",
    "print('words:', vocab.to_tokens(indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def6984a-c182-4eeb-aa6a-4d7179e5aed6",
   "metadata": {},
   "source": [
    "Lastly, create a function to extract the corpus of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1278d25e-12ca-4187-b9d6-4c1137a48c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(TimeMachine)  #@save\n",
    "def build(self, raw_text, vocab=None):\n",
    "    tokens = self._tokenize(self._preprocess(raw_text))\n",
    "    if vocab is None: vocab = Vocab(tokens)\n",
    "    corpus = [vocab[token] for token in tokens]\n",
    "    return corpus, vocab\n",
    "\n",
    "corpus, vocab = data.build(raw_text)\n",
    "len(corpus), len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b8be06-c70d-4194-98dc-792aebd00b75",
   "metadata": {},
   "source": [
    "Language models are commonly used to perform explanatory statistics, which can be used to define some properties of the corpus of the text, such as the most common words in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81ee881-91de-4d21-a305-dbcdfa0cb65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = text.split()\n",
    "vocab = Vocab(words)\n",
    "vocab.token_freqs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8db5a0-1e39-4fa4-924d-9e958b2a5fac",
   "metadata": {},
   "source": [
    "Overall, the main issue of this technique lies in the fact that it often captures stop words, such as \"I\" or \"The\", which are not descriptive. <br>\n",
    "In any case, it is possible to notice that word frequency follows a Zipfian Power law distribution as it quickly decays going down the list. <br>\n",
    "Therefore, it is possible to formulate the frequency $n_i$ of the $i^{\\text{th}}$ most frequent word as $n_i \\propto \\frac{1}{i^\\alpha}$, where $\\alpha$ is an exponent associated to the Power law distribution. <br>\n",
    "**N.B:** Alternatively, it is possible to interpret word frequency as $\\log n_i = -\\alpha\\log i + c$, where $c$ is a constant term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eef64ea-a7ed-496d-aa5a-b9fc0e734161",
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = [freq for token, freq in vocab.token_freqs]\n",
    "d2l.plot(freqs, xlabel='token: x', ylabel='frequency: n(x)',\n",
    "         xscale='log', yscale='log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8696ba6-3815-4f0e-a4f4-26eb4ead4a14",
   "metadata": {},
   "source": [
    "Note that it is possible to perform further analysis by analyzing word bigrams or trigrams, although this research might be more expensive to carry out in practice. <br>\n",
    "This is caused by the fact that bigrams and trigrams tend to make more unique tokens compared to single words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48231257-cbcf-4b34-94a9-041a3ac42a48",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d087d038-61a7-401a-8be8-82ffc81d5a48",
   "metadata": {},
   "source": [
    "A recurrent neural network is characterised by the presence of a hiden state that gets updated during computations using a recurrence based on the current input and on the previous hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d2fbf2-637e-4a22-b755-458106d5f9d4",
   "metadata": {},
   "source": [
    "The most common recurrent neural network model is the Elman RNN, where $h_t = \\tanh(W_{hh}h_{t - 1} + W_{xh}x_t)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa924676-0a95-46bc-bd11-61a0c06a9eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNScratch(d2l.Module):\n",
    "    \"\"\"The RNN model implemented from scratch.\"\"\"\n",
    "    def __init__(self, num_inputs, num_hiddens, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.W_xh = nn.Parameter(\n",
    "            torch.randn(num_inputs, num_hiddens) * sigma)\n",
    "        self.W_hh = nn.Parameter(\n",
    "            torch.randn(num_hiddens, num_hiddens) * sigma)\n",
    "        self.b_h = nn.Parameter(torch.zeros(num_hiddens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5f5252-ec01-4147-8dd4-40a587b4f30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(RNNScratch)\n",
    "def forward(self, inputs, state=None):\n",
    "    if state is None:\n",
    "        # Initial state with shape: (batch_size, num_hiddens)\n",
    "        state = torch.zeros((inputs.shape[1], self.num_hiddens),\n",
    "                          device=inputs.device)\n",
    "    else:\n",
    "        state, = state\n",
    "    outputs = []\n",
    "    for X in inputs:  # Shape of inputs: (num_steps, batch_size, num_inputs).\n",
    "        state = torch.tanh(torch.matmul(X, self.W_xh) +\n",
    "                         torch.matmul(state, self.W_hh) + self.b_h)\n",
    "        outputs.append(state)\n",
    "    return outputs, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5452d357-40a5-46de-8a80-b6704d571464",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, num_inputs, num_hiddens, num_steps = 2, 16, 32, 100\n",
    "rnn = RNNScratch(num_inputs, num_hiddens)\n",
    "X = torch.ones((num_steps, batch_size, num_inputs))\n",
    "outputs, state = rnn(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1bfb72-e943-47e2-8ffd-36ee0773bf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLMScratch(d2l.Classifier):\n",
    "    \"\"\"The RNN-based language model implemented from scratch.\"\"\"\n",
    "    def __init__(self, rnn, vocab_size, lr=0.01):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        self.W_hq = nn.Parameter(\n",
    "            torch.randn(\n",
    "                self.rnn.num_hiddens, self.vocab_size) * self.rnn.sigma)\n",
    "        self.b_q = nn.Parameter(torch.zeros(self.vocab_size))\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        l = self.loss(self(*batch[:-1]), batch[-1])\n",
    "        self.plot('ppl', torch.exp(l), train=True)\n",
    "        return l\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        l = self.loss(self(*batch[:-1]), batch[-1])\n",
    "        self.plot('ppl', torch.exp(l), train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc55a0a-d4d0-4ba5-9dee-c64d10cd0ab3",
   "metadata": {},
   "source": [
    "In the context of language processing, it is possible to efficiently represent the input using one-hot encoding in terms of its position within the text vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921c79b4-57d7-4735-9615-e5b074271e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.one_hot(torch.tensor([0, 2]), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7727ffcd-11da-4c7f-b157-e8d51d55ad1e",
   "metadata": {},
   "source": [
    "Train the model on H.G. Wells' *The Time Machine*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5955404b-04e8-481d-af5f-e5c60d4029a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = d2l.TimeMachine(batch_size=1024, num_steps=32)\n",
    "rnn = RNNScratch(num_inputs=len(data.vocab), num_hiddens=32)\n",
    "model = RNNLMScratch(rnn, vocab_size=len(data.vocab), lr=1)\n",
    "trainer = d2l.Trainer(max_epochs=100, gradient_clip_val=1, num_gpus=1)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aa711e-cbd0-40c9-948e-d3fd7e8f8739",
   "metadata": {},
   "source": [
    "## Long Short-Term Memory Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2691b49f-862f-42a6-b03d-83772604795e",
   "metadata": {},
   "source": [
    "One of the main issues of recurrent neural networks lies in both the vanishing gradient problem and in the memory requirements for the dependencies. <br>\n",
    "A LSTM model thus tries to overcome these issues by implementing some learnable gates, such as input, which indicates the importance of the input, forget, which indicates whether stored information should be kept or overwritten, and output, which indicates the importance of the output signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7175af-68fa-40fa-8867-87ebebd42495",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMScratch(d2l.Module):\n",
    "    def __init__(self, num_inputs, num_hiddens, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        init_weight = lambda *shape: nn.Parameter(torch.randn(*shape) * sigma)\n",
    "        triple = lambda: (init_weight(num_inputs, num_hiddens),\n",
    "                          init_weight(num_hiddens, num_hiddens),\n",
    "                          nn.Parameter(torch.zeros(num_hiddens)))\n",
    "        self.W_xi, self.W_hi, self.b_i = triple()  # Input gate.\n",
    "        self.W_xf, self.W_hf, self.b_f = triple()  # Forget gate.\n",
    "        self.W_xo, self.W_ho, self.b_o = triple()  # Output gate.\n",
    "        self.W_xc, self.W_hc, self.b_c = triple()  # Input node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41994459-d363-43ec-98a1-f23ebcbe7d0f",
   "metadata": {},
   "source": [
    "From a high-level perspective, a LSTM model will make use of both a short-term memory, which handles hidden states, and a long-term memory, which handles cell states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d472cc9-ac00-4ef5-a97d-239427cd2583",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(LSTMScratch)\n",
    "def forward(self, inputs, H_C=None):\n",
    "    if H_C is None:\n",
    "        # Initial state with shape: (batch_size, num_hiddens)\n",
    "        H = torch.zeros((inputs.shape[1], self.num_hiddens),\n",
    "                      device=inputs.device)\n",
    "        C = torch.zeros((inputs.shape[1], self.num_hiddens),\n",
    "                      device=inputs.device)\n",
    "    else:\n",
    "        H, C = H_C\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "        I = torch.sigmoid(torch.matmul(X, self.W_xi) +\n",
    "                        torch.matmul(H, self.W_hi) + self.b_i)\n",
    "        F = torch.sigmoid(torch.matmul(X, self.W_xf) +\n",
    "                        torch.matmul(H, self.W_hf) + self.b_f)\n",
    "        O = torch.sigmoid(torch.matmul(X, self.W_xo) +\n",
    "                        torch.matmul(H, self.W_ho) + self.b_o)\n",
    "        C_tilde = torch.tanh(torch.matmul(X, self.W_xc) +\n",
    "                           torch.matmul(H, self.W_hc) + self.b_c)\n",
    "        C = F * C + I * C_tilde\n",
    "        H = O * torch.tanh(C)\n",
    "        outputs.append(H)\n",
    "    return outputs, (H, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf9c909-feaf-4d83-ae6a-d6dc4c5bfcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = d2l.TimeMachine(batch_size=1024, num_steps=32)\n",
    "lstm = LSTMScratch(num_inputs=len(data.vocab), num_hiddens=32)\n",
    "model = d2l.RNNLMScratch(lstm, vocab_size=len(data.vocab), lr=0.01)\n",
    "trainer = d2l.Trainer(max_epochs=50, gradient_clip_val=1, num_gpus=1)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57def6a5-9ace-4834-8b14-a6f4c62e3769",
   "metadata": {},
   "source": [
    "Using dedicated APIs, it is possible to provide a more concise implementation of a LSTM network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911c2ff9-160d-4acf-a758-de089e7e3903",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(d2l.RNN):\n",
    "    def __init__(self, num_inputs, num_hiddens):\n",
    "        d2l.Module.__init__(self)\n",
    "        self.save_hyperparameters()\n",
    "        self.rnn = nn.LSTM(num_inputs, num_hiddens)\n",
    "\n",
    "    def forward(self, inputs, H_C=None):\n",
    "        return self.rnn(inputs, H_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16170e9a-21f9-496d-81af-b2335f36561f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTM(num_inputs=len(data.vocab), num_hiddens=32)\n",
    "model = d2l.RNNLM(lstm, vocab_size=len(data.vocab), lr=0.01)\n",
    "trainer.fit(model, data)\n",
    "\n",
    "model.predict('it has', 20, data.vocab, d2l.try_gpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ae4adf-3f44-4bc1-8f2a-73937671ff1a",
   "metadata": {},
   "source": [
    "These models can also be used for machine translation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feca8228-efd4-4f96-a1e8-0a90b9c03ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTFraEng(d2l.DataModule):\n",
    "    \"\"\"The English-French dataset.\"\"\"\n",
    "    def _download(self):\n",
    "        d2l.extract(d2l.download(\n",
    "            d2l.DATA_URL+'fra-eng.zip', self.root,\n",
    "            '94646ad1522d915e7b0f9296181140edcf86a4f5'))\n",
    "        with open(self.root + '/fra-eng/fra.txt', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "\n",
    "data = MTFraEng()\n",
    "raw_text = data._download()\n",
    "print(raw_text[:75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496bffba-4d8b-4182-bf29-8e8fa2d358d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(MTFraEng)\n",
    "def _preprocess(self, text):\n",
    "    # Replace non-breaking space with space.\n",
    "    text = text.replace('\\u202f', ' ').replace('\\xa0', ' ')\n",
    "    # Insert space between words and punctuation marks.\n",
    "    no_space = lambda char, prev_char: char in ',.!?' and prev_char != ' '\n",
    "    out = [' ' + char if i > 0 and no_space(char, text[i - 1]) else char\n",
    "           for i, char in enumerate(text.lower())]\n",
    "    return ''.join(out)\n",
    "\n",
    "text = data._preprocess(raw_text)\n",
    "print(text[:80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788c6099-954a-4382-a0f8-a0e18574968f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(MTFraEng)\n",
    "def _tokenize(self, text, max_examples=None):\n",
    "    src, tgt = [], []\n",
    "    for i, line in enumerate(text.split('\\n')):\n",
    "        if max_examples and i > max_examples: break\n",
    "        parts = line.split('\\t')\n",
    "        if len(parts) == 2:\n",
    "            # Skip empty tokens.\n",
    "            src.append([t for t in f'{parts[0]} <eos>'.split(' ') if t])\n",
    "            tgt.append([t for t in f'{parts[1]} <eos>'.split(' ') if t])\n",
    "    return src, tgt\n",
    "\n",
    "src, tgt = data._tokenize(text)\n",
    "src[:6], tgt[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625521e7-c7f5-4bbb-8d3e-3386a7ac9393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_list_len_pair_hist(legend, xlabel, ylabel, xlist, ylist):\n",
    "    \"\"\"Plot the histogram for list length pairs.\"\"\"\n",
    "    d2l.set_figsize()\n",
    "    _, _, patches = d2l.plt.hist(\n",
    "        [[len(l) for l in xlist], [len(l) for l in ylist]])\n",
    "    d2l.plt.xlabel(xlabel)\n",
    "    d2l.plt.ylabel(ylabel)\n",
    "    for patch in patches[1].patches:\n",
    "        patch.set_hatch('/')\n",
    "    d2l.plt.legend(legend)\n",
    "\n",
    "show_list_len_pair_hist(['source', 'target'], '# tokens per sequence', 'count', src, tgt) # Plot the histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37eeed07-49e9-46bc-b695-0fffb012e2b6",
   "metadata": {},
   "source": [
    "## The Encoder-Decoder Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5f9d8f-21bc-4f2b-910c-418d4d574fb1",
   "metadata": {},
   "source": [
    "Generally speaking, machine translation tasks can be performed using a many-to-one encoder, which processes a sequence of inputs to produce a latent vector, and a one-to-many decoder, which uses the latent vector to produce a sequence of outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e7e2fd-5d22-446c-ba0b-acb0ded95d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"The base encoder interface for the encoder--decoder architecture.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    # Later there can be additional arguments (e.g., length excluding padding).\n",
    "    def forward(self, X, *args):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1654fc-ea46-45d0-9f63-00379a856145",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"The base decoder interface for the encoder--decoder architecture.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    # Later there can be additional arguments (e.g., length excluding padding).\n",
    "    def init_state(self, enc_all_outputs, *args):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dd3a1b-9cf6-4925-8b6e-f3189b7413fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(d2l.Classifier):\n",
    "    \"\"\"The base class for the encoder--decoder architecture.\"\"\"\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, enc_X, dec_X, *args):\n",
    "        enc_all_outputs = self.encoder(enc_X, *args)\n",
    "        dec_state = self.decoder.init_state(enc_all_outputs, *args)\n",
    "        # Return decoder output only.\n",
    "        return self.decoder(dec_X, dec_state)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a921a0a-95e3-4d1b-bbd6-55c7efd86421",
   "metadata": {},
   "source": [
    "Today, this architecture is commonly implemented as a sequence-to-sequence model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3717b2-854d-44c4-8d1c-c5c6cb8d2b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_seq2seq(module):\n",
    "    \"\"\"Initialize weights for sequence-to-sequence learning.\"\"\"\n",
    "    if type(module) == nn.Linear:\n",
    "         nn.init.xavier_uniform_(module.weight)\n",
    "    if type(module) == nn.GRU:\n",
    "        for param in module._flat_weights_names:\n",
    "            if \"weight\" in param:\n",
    "                nn.init.xavier_uniform_(module._parameters[param])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2229691-900d-48e3-8264-35da808c9120",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqEncoder(d2l.Encoder):\n",
    "    \"\"\"The RNN encoder for sequence-to-sequence learning.\"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = d2l.GRU(embed_size, num_hiddens, num_layers, dropout)\n",
    "        self.apply(init_seq2seq)\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        # X shape: (batch_size, num_steps)\n",
    "        embs = self.embedding(X.t().type(torch.int64))\n",
    "        # embs shape: (num_steps, batch_size, embed_size)\n",
    "        outputs, state = self.rnn(embs)\n",
    "        # outputs shape: (num_steps, batch_size, num_hiddens)\n",
    "        # state shape: (num_layers, batch_size, num_hiddens)\n",
    "        return outputs, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95714c97-45c6-49a9-b8b8-ad4bf613246c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size, embed_size, num_hiddens, num_layers = 10, 8, 16, 2\n",
    "batch_size, num_steps = 4, 9\n",
    "encoder = Seq2SeqEncoder(vocab_size, embed_size, num_hiddens, num_layers)\n",
    "X = torch.zeros((batch_size, num_steps))\n",
    "enc_outputs, enc_state = encoder(X)\n",
    "d2l.check_shape(enc_outputs, (num_steps, batch_size, num_hiddens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f61658-ba8e-4212-8c2d-5871bc3af38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDecoder(d2l.Decoder):\n",
    "    \"\"\"The RNN decoder for sequence to sequence learning.\"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = d2l.GRU(embed_size+num_hiddens, num_hiddens,\n",
    "                           num_layers, dropout)\n",
    "        self.dense = nn.LazyLinear(vocab_size)\n",
    "        self.apply(init_seq2seq)\n",
    "\n",
    "    def init_state(self, enc_all_outputs, *args):\n",
    "        return enc_all_outputs\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        # X shape: (batch_size, num_steps)\n",
    "        # embs shape: (num_steps, batch_size, embed_size)\n",
    "        embs = self.embedding(X.t().type(torch.int32))\n",
    "        enc_output, hidden_state = state\n",
    "        # context shape: (batch_size, num_hiddens)\n",
    "        context = enc_output[-1]\n",
    "        # Broadcast context to (num_steps, batch_size, num_hiddens)\n",
    "        context = context.repeat(embs.shape[0], 1, 1)\n",
    "        # Concat at the feature dimension\n",
    "        embs_and_context = torch.cat((embs, context), -1)\n",
    "        outputs, hidden_state = self.rnn(embs_and_context, hidden_state)\n",
    "        outputs = self.dense(outputs).swapaxes(0, 1)\n",
    "        # outputs shape: (batch_size, num_steps, vocab_size)\n",
    "        # hidden_state shape: (num_layers, batch_size, num_hiddens)\n",
    "        return outputs, [enc_output, hidden_state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353d6bfb-fb64-43ab-b7bd-f263949478fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Seq2SeqDecoder(vocab_size, embed_size, num_hiddens, num_layers)\n",
    "state = decoder.init_state(encoder(X))\n",
    "dec_outputs, state = decoder(X, state)\n",
    "d2l.check_shape(dec_outputs, (batch_size, num_steps, vocab_size))\n",
    "d2l.check_shape(state[1], (num_layers, batch_size, num_hiddens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4917bf1-b9a2-4342-bf94-49daca0f19b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(d2l.EncoderDecoder):  #@save\n",
    "    \"\"\"The RNN encoder--decoder for sequence to sequence learning.\"\"\"\n",
    "    def __init__(self, encoder, decoder, tgt_pad, lr):\n",
    "        super().__init__(encoder, decoder)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        Y_hat = self(*batch[:-1])\n",
    "        self.plot('loss', self.loss(Y_hat, batch[-1]), train=False)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Adam optimizer is used here\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a26bce9-c893-4f61-9b8d-65d7e825983b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = d2l.MTFraEng(batch_size=128)\n",
    "embed_size, num_hiddens, num_layers, dropout = 256, 256, 2, 0.2\n",
    "encoder = Seq2SeqEncoder(\n",
    "    len(data.src_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "decoder = Seq2SeqDecoder(\n",
    "    len(data.tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "model = Seq2Seq(encoder, decoder, tgt_pad=data.tgt_vocab['<pad>'],\n",
    "                lr=0.005)\n",
    "trainer = d2l.Trainer(max_epochs=30, gradient_clip_val=1, num_gpus=1)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62fb8a1-fb2e-400e-9c36-f5b8b6309883",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(d2l.EncoderDecoder)\n",
    "def predict_step(self, batch, device, num_steps,\n",
    "                 save_attention_weights=False):\n",
    "    batch = [a.to(device) for a in batch]\n",
    "    src, tgt, src_valid_len, _ = batch\n",
    "    enc_all_outputs = self.encoder(src, src_valid_len)\n",
    "    dec_state = self.decoder.init_state(enc_all_outputs, src_valid_len)\n",
    "    outputs, attention_weights = [tgt[:, (0)].unsqueeze(1), ], []\n",
    "    for _ in range(num_steps):\n",
    "        Y, dec_state = self.decoder(outputs[-1], dec_state)\n",
    "        outputs.append(Y.argmax(2))\n",
    "        # Save attention weights (to be covered later)\n",
    "        if save_attention_weights:\n",
    "            attention_weights.append(self.decoder.attention_weights)\n",
    "    return torch.cat(outputs[1:], 1), attention_weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
