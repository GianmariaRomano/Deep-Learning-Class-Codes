{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4682fd66-863f-4301-9715-45a484d63b45",
   "metadata": {},
   "source": [
    "# Transformer Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7ac646-81fb-4751-bc7f-1667196d1822",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install d2l==1.0.3\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e253f8-9ee4-46b6-9064-8a17a99f4577",
   "metadata": {},
   "source": [
    "## Queries, Keys, Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377b3225-f699-4709-bb2a-f31bdb08bfaa",
   "metadata": {},
   "source": [
    "Given a database $\\mathcal{D} = \\{(k_1, v_1), \\dots, (k_m, v_m)\\}$ containing $m$ key-value tuples and a query $q$, it is possible to define the attention of the query over the database as $A(q, \\mathcal{D}) = \\sum_{i = 1}^m \\alpha(q, k_i)v_i$, where $\\alpha$ denotes a collection of normalized attention weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28a156c-e921-4513-9403-6c7416b69ae5",
   "metadata": {},
   "source": [
    "## Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5045e4-9262-4f7d-8249-e45a85fe9ed5",
   "metadata": {},
   "source": [
    "Multi-head attention comes in handy to extract different features by implementing different attention heads working in parallel and concatenating each result to obtain the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38cb054-7080-498b-8c6d-39f04ccbe57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(d2l.Module):\n",
    "    def __init__(self, num_hiddens, num_heads, dropout, bias=False, **kwargs):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = d2l.DotProductAttention(dropout)\n",
    "        self.W_q = nn.LazyLinear(num_hiddens, bias=bias)\n",
    "        self.W_k = nn.LazyLinear(num_hiddens, bias=bias)\n",
    "        self.W_v = nn.LazyLinear(num_hiddens, bias=bias)\n",
    "        self.W_o = nn.LazyLinear(num_hiddens, bias=bias)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens):\n",
    "        '''\n",
    "        Shape of queries, keys, or values: (batch_size, no. of queries or key-value pairs, num_hiddens).\n",
    "        Shape of valid_lens: (batch_size,) or (batch_size, no. of queries).\n",
    "        After transposing, shape of output queries, keys, or values: (batch_size * num_heads, no. of queries or key-value pairs, num_hiddens / num_heads).\n",
    "        '''\n",
    "        queries = self.transpose_qkv(self.W_q(queries))\n",
    "        keys = self.transpose_qkv(self.W_k(keys))\n",
    "        values = self.transpose_qkv(self.W_v(values))\n",
    "\n",
    "        if valid_lens is not None:\n",
    "            '''\n",
    "            On axis 0, copy the first item for num_heads times.\n",
    "            Then, copy the next item, and so on.\n",
    "            '''\n",
    "            valid_lens = torch.repeat_interleave(\n",
    "                valid_lens, repeats=self.num_heads, dim=0)\n",
    "\n",
    "        '''\n",
    "        Shape of output: (batch size * num_heads, no. of queries, num_hiddens / num_heads).\n",
    "        Shape of output_concat: (batch_size, no. of queries, num_hiddens).\n",
    "        '''\n",
    "        output = self.attention(queries, keys, values, valid_lens)\n",
    "        output_concat = self.transpose_output(output)\n",
    "        return self.W_o(output_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cad999-f68c-4b11-9e71-5db98a6a459b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(MultiHeadAttention)\n",
    "\n",
    "# Transposition for parallel computation of multiple attention heads.\n",
    "def transpose_qkv(self, X):\n",
    "    '''\n",
    "    Shape of input X: (batch_size, no. of queries or key-value pairs, num_hiddens).\n",
    "    Shape of output X: (batch_size, no. of queries or key-value pairs, num_heads, num_hiddens / num_heads)\n",
    "    '''\n",
    "    X = X.reshape(X.shape[0], X.shape[1], self.num_heads, -1)\n",
    "    '''\n",
    "    Shape of output X: (batch_size, num_heads, no. of queries or key-value pairs, num_hiddens / num_heads).\n",
    "    '''\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "    '''\n",
    "    Shape of output: (batch_size * num_heads, no. of queries or key-value pairs, num_hiddens / num_heads).\n",
    "    '''\n",
    "    return X.reshape(-1, X.shape[2], X.shape[3])\n",
    "\n",
    "# Reverse the operation defined in transpose_qkv.\n",
    "@d2l.add_to_class(MultiHeadAttention)\n",
    "def transpose_output(self, X):\n",
    "    X = X.reshape(-1, self.num_heads, X.shape[1], X.shape[2])\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "    return X.reshape(X.shape[0], X.shape[1], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e068a983-be6d-405c-be3b-f3e9a3836438",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hiddens, num_heads = 100, 5\n",
    "attention = MultiHeadAttention(num_hiddens, num_heads, 0.5)\n",
    "batch_size, num_queries, num_kvpairs = 2, 4, 6\n",
    "valid_lens = torch.tensor([3, 2])\n",
    "X = torch.ones((batch_size, num_queries, num_hiddens))\n",
    "Y = torch.ones((batch_size, num_kvpairs, num_hiddens))\n",
    "d2l.check_shape(attention(X, Y, Y, valid_lens), (batch_size, num_queries, num_hiddens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409e88ff-358d-4ae2-ac07-fd2995420bc7",
   "metadata": {},
   "source": [
    "## Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6c3468-7691-4587-8920-947a3520efff",
   "metadata": {},
   "source": [
    "Self-attention makes comparisons between input vectors. <br>\n",
    "However, since self-attention is permutation invariant, it might struggle on some tasks where the order of the input sequence matters. <br>\n",
    "For this reason, it can come in handy, for each input $x_j$, to append a positional encoding $p_j = pos(j)$ that is defined using a function $pos: \\mathbb{N} \\to \\mathbb{R}^d$, which can be learned using lookup tables or fixed a priori."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a711f100-9e16-44cf-98a4-1531e0297228",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, num_hiddens, dropout, max_len=1000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens)) # Create a long enough P.\n",
    "        X = torch.arange(max_len, dtype=torch.float32).reshape(1, 1) / torch.pow(10000, torch.arange(0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
    "        self.P[:, :, 0::2] = torch.sin(X)\n",
    "        self.P[:, :, 1::2] = torch.cos(X)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X + self.P[:, :X.shape[1], :].to(X.device)\n",
    "        return self.dropout(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a285447-2546-434f-9801-ad4fd2c5d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_dim, num_steps = 32, 60\n",
    "pos_encoding = PositionalEncoding(encoding_dim, 0)\n",
    "X = pos_encoding(torch.zeros((1, num_steps, encoding_dim)))\n",
    "P = pos_encoding.P[:, :X.shape[1], :]\n",
    "d2l.plot(torch.arange(num_steps), P[0, :, 6:10].T, xlabel='Row (position)', figsize=(6, 2.5), legend=[\"Col %d\" % d for d in torch.arange(6, 10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1583a4-78ad-4541-9021-c52e1008ac10",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c031446-4790-4d6c-bc25-b1d8a1ad7b0a",
   "metadata": {},
   "source": [
    "Commonly used for natural language processing tasks, a transformer is a particular instance of the encoder-decoder architecture. <br>\n",
    "In particular, the encoder takes the input data and processes the corresponding positional encoding using a multi-head self-attention layer (eventually adding a residual connection to handle gradient flows) and performing computations via layer normalization, a multilayer perceptron and another layer normalization before producing the output. <br>\n",
    "The decoder works similarly as the encoder, but it implements both a masked multi-head attention layer and a generic multi-head attention layer for the encoder outputs. <br>\n",
    "Overall, transformers are highly scalable and parallelizable, although they require much more memory to store the needed parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb48c17-8e35-438e-82ed-4c6d47c93f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFFN(nn.Module):\n",
    "    def __init__(self, ffn_num_hiddens, ffn_num_outputs):\n",
    "        super().__init__()\n",
    "        self.dense1 = nn.LazyLinear(ffn_num_hiddens)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dense2 = nn.LazyLinear(ffn_num_outputs)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.dense2(self.relu(self.dense1(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1516654-6239-40ab-81c8-b6d44fc63906",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNorm(nn.Module): \n",
    "    def __init__(self, norm_shape, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln = nn.LayerNorm(norm_shape)\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        return self.ln(self.dropout(Y) + X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea4cc2c-52d5-4a95-9539-e35af352d6b3",
   "metadata": {},
   "source": [
    "Start by creating the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ff4f0a-f37b-4f51-8615-278e24d0a331",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, num_hiddens, ffn_num_hiddens, num_heads, dropout, use_bias=False):\n",
    "        super().__init__()\n",
    "        self.attention = d2l.MultiHeadAttention(num_hiddens, num_heads, dropout, use_bias)\n",
    "        self.addnorm1 = AddNorm(num_hiddens, dropout)\n",
    "        self.ffn = PositionWiseFFN(ffn_num_hiddens, num_hiddens)\n",
    "        self.addnorm2 = AddNorm(num_hiddens, dropout)\n",
    "\n",
    "    def forward(self, X, valid_lens):\n",
    "        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))\n",
    "        return self.addnorm2(Y, self.ffn(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d066ac37-0833-41f4-85fb-5dc7dd158dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(d2l.Encoder):\n",
    "    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens, num_heads, num_blks, dropout, use_bias=False):\n",
    "        super().__init__()\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_blks):\n",
    "            self.blks.add_module(\"block\"+str(i), TransformerEncoderBlock(\n",
    "                num_hiddens, ffn_num_hiddens, num_heads, dropout, use_bias))\n",
    "\n",
    "    def forward(self, X, valid_lens):\n",
    "        '''\n",
    "        Since positional encoding values are between -1 and 1, the embedding\n",
    "        values are multiplied by the square root of the embedding dimension\n",
    "        to rescale before they are summed up.\n",
    "        '''\n",
    "        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))\n",
    "        self.attention_weights = [None] * len(self.blks)\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X = blk(X, valid_lens)\n",
    "            self.attention_weights[i] = blk.attention.attention.attention_weights\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2e0ce3-f1b4-4c27-9fda-e007019d3f3c",
   "metadata": {},
   "source": [
    "Then, implement the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b55502-63d9-4dc7-b0b7-be20e0b3a5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderBlock(nn.Module):\n",
    "    def __init__(self, num_hiddens, ffn_num_hiddens, num_heads, dropout, i):\n",
    "        super().__init__()\n",
    "        self.i = i\n",
    "        self.attention1 = d2l.MultiHeadAttention(num_hiddens, num_heads, dropout)\n",
    "        self.addnorm1 = AddNorm(num_hiddens, dropout)\n",
    "        self.attention2 = d2l.MultiHeadAttention(num_hiddens, num_heads, dropout)\n",
    "        self.addnorm2 = AddNorm(num_hiddens, dropout)\n",
    "        self.ffn = PositionWiseFFN(ffn_num_hiddens, num_hiddens)\n",
    "        self.addnorm3 = AddNorm(num_hiddens, dropout)\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        enc_outputs, enc_valid_lens = state[0], state[1]\n",
    "        '''\n",
    "        During training, all the tokens of any output sequence are processed\n",
    "        at the same time, so state[2][self.i] is None as initialized.\n",
    "        When decoding any output sequence token by token during prediction,\n",
    "        state[2][self.i] contains representations of the decoded output at\n",
    "        the i-th block up to the current time step.\n",
    "        '''\n",
    "        if state[2][self.i] is None:\n",
    "            key_values = X\n",
    "        else:\n",
    "            key_values = torch.cat((state[2][self.i], X), dim=1)\n",
    "        state[2][self.i] = key_values\n",
    "        if self.training:\n",
    "            batch_size, num_steps, _ = X.shape\n",
    "            '''\n",
    "            Shape of dec_valid_lens: (batch_size, num_steps), where every row is [1, 2, ..., num_steps].\n",
    "            '''\n",
    "            dec_valid_lens = torch.arange(\n",
    "                1, num_steps + 1, device=X.device).repeat(batch_size, 1)\n",
    "        else:\n",
    "            dec_valid_lens = None\n",
    "\n",
    "        X2 = self.attention1(X, key_values, key_values, dec_valid_lens) # Self-attention.\n",
    "        Y = self.addnorm1(X, X2)\n",
    "        Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens) # Encoder-decoder self-attention.\n",
    "        Z = self.addnorm2(Y, Y2)\n",
    "        return self.addnorm3(Z, self.ffn(Z)), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adb21ad-67f5-4013-807b-27813d38ce52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(d2l.AttentionDecoder):\n",
    "    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens, num_heads, num_blks, dropout):\n",
    "        super().__init__()\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_blks = num_blks\n",
    "        self.embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_blks):\n",
    "            self.blks.add_module(\"block\"+str(i), TransformerDecoderBlock(num_hiddens, ffn_num_hiddens, num_heads, dropout, i))\n",
    "        self.dense = nn.LazyLinear(vocab_size)\n",
    "\n",
    "    def init_state(self, enc_outputs, enc_valid_lens):\n",
    "        return [enc_outputs, enc_valid_lens, [None] * self.num_blks]\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))\n",
    "        self._attention_weights = [[None] * len(self.blks) for _ in range (2)]\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X, state = blk(X, state)\n",
    "            self._attention_weights[0][i] = blk.attention1.attention.attention_weights # Decoder self-attention weights.\n",
    "            self._attention_weights[1][i] = blk.attention2.attention.attention_weights # Encoder-decoder self-attention weights.\n",
    "        return self.dense(X), state\n",
    "\n",
    "    @property\n",
    "    def attention_weights(self):\n",
    "        return self._attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51b3ffa-c10f-4f1b-84d6-3bc0c7f72f05",
   "metadata": {},
   "source": [
    "Having created the transformer, try to train it. <br>\n",
    "Then, after training, use the model to try and translate some sentences to English to French."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de60b7b0-967c-41c1-bf3b-cae145712f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "ata = d2l.MTFraEng(batch_size=128)\n",
    "num_hiddens, num_blks, dropout = 256, 2, 0.2\n",
    "ffn_num_hiddens, num_heads = 64, 4\n",
    "encoder = TransformerEncoder(len(data.src_vocab), num_hiddens, ffn_num_hiddens, num_heads, num_blks, dropout)\n",
    "decoder = TransformerDecoder(len(data.tgt_vocab), num_hiddens, ffn_num_hiddens, num_heads, num_blks, dropout)\n",
    "model = d2l.Seq2Seq(encoder, decoder, tgt_pad=data.tgt_vocab['<pad>'], lr=0.001)\n",
    "trainer = d2l.Trainer(max_epochs=30, gradient_clip_val=1, num_gpus=1)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bcc62d-3610-4ae4-aa80-510f9c0ccd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "engs = ['go .', 'i lost .', 'he\\'s calm .', 'i\\'m home .']\n",
    "fras = ['va !', 'j\\'ai perdu .', 'il est calme .', 'je suis chez moi .']\n",
    "preds, _ = model.predict_step(data.build(engs, fras), d2l.try_gpu(), data.num_steps)\n",
    "\n",
    "for en, fr, p in zip(engs, fras, preds):\n",
    "    translation = []\n",
    "    for token in data.tgt_vocab.to_tokens(p):\n",
    "        if token == '<eos>':\n",
    "            break\n",
    "        translation.append(token)\n",
    "    print(f'{en} => {translation}, bleu,'\n",
    "          f'{d2l.bleu(\" \".join(translation), fr, k=2):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c12401-e3e3-495e-a11c-0441f2b88440",
   "metadata": {},
   "source": [
    "Show the encoder and decoder weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1861cb3c-2b62-4699-97c8-fc5adf725530",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, dec_attention_weights = model.predict_step(data.build([engs[-1]], [fras[-1]]), d2l.try_gpu(), data.num_steps, True)\n",
    "enc_attention_weights = torch.cat(model.encoder.attention_weights, 0)\n",
    "shape = (num_blks, num_heads, -1, data.num_steps)\n",
    "enc_attention_weights = enc_attention_weights.reshape(shape)\n",
    "d2l.check_shape(enc_attention_weights, (num_blks, num_heads, data.num_steps, data.num_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527462d8-2888-4c33-b1a8-c860053c94cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.show_heatmaps(enc_attention_weights.cpu(), xlabel='Key positions', ylabel='Query positions', titles=['Head %d' % i for i in range(1, 5)], figsize=(7, 3.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dec7d5-fe38-42a4-9da5-dfaeace0c5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_attention_weights_2d = [head[0].tolist() for step in dec_attention_weights for attn in step for blk in attn for head in blk]\n",
    "dec_attention_weights_filled = torch.tensor(pd.DataFrame(dec_attention_weights_2d).fillna(0.0).values)\n",
    "shape = (-1, 2, num_blks, num_heads, data.num_steps)\n",
    "dec_attention_weights = dec_attention_weights_filled.reshape(shape)\n",
    "dec_self_attention_weights, dec_inter_attention_weights = \\ dec_attention_weights.permute(1, 2, 3, 0, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9817cd9b-074c-4d4c-85c0-42dd161c40e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.check_shape(dec_self_attention_weights, (num_blks, num_heads, data.num_steps, data.num_steps))\n",
    "d2l.check_shape(dec_inter_attention_weights, (num_blks, num_heads, data.num_steps, data.num_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca85094-9cab-4b75-abf1-42036a0a5664",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.show_heatmaps(dec_self_attention_weights[:, :, :, :], xlabel='Key positions', ylabel='Query positions', titles=['Head %d' % i for i in range(1, 5)], figsize=(7, 3.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09d8ce3-94c6-43be-8b4e-68c4e4c81e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.show_heatmaps(dec_inter_attention_weights, xlabel='Key positions', ylabel='Query positions', titles=['Head %d' % i for i in range(1, 5)], figsize=(7, 3.5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
